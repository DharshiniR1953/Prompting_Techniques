{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzbQLdVD2Ivn"
      },
      "source": [
        "# Few-Shot Learning\n",
        "\n",
        "## OverviewðŸ˜Š\n",
        "Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwjSDs8y2Ivp",
        "outputId": "4cdb2cc0-edf1-466c-dce3-933c70493d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",openai_api_key=\"Place your openai api key hereðŸ˜Š.\")\n",
        "\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yVbOwJo2Ivr"
      },
      "source": [
        "## Basic Few-Shot Learning\n",
        "\n",
        "\n",
        "Sentiment Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5tSsnjD2Ivr",
        "outputId": "06e0d62f-0e7d-49de-9366-b6d0d81f9d02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: I can't believe how great this new restaurant is!\n",
            "Predicted Sentiment: Positive\n"
          ]
        }
      ],
      "source": [
        "def few_shot_sentiment_classification(input_text):\n",
        "    few_shot_prompt = PromptTemplate(\n",
        "        input_variables=[\"input_text\"],\n",
        "        template=\"\"\"\n",
        "        Classify the sentiment as Positive, Negative, or Neutral.\n",
        "\n",
        "        Examples:\n",
        "        Text: I love this product! It's amazing.\n",
        "        Sentiment: Positive\n",
        "\n",
        "        Text: This movie was terrible. I hated it.\n",
        "        Sentiment: Negative\n",
        "\n",
        "        Text: The weather today is okay.\n",
        "        Sentiment: Neutral\n",
        "\n",
        "        Now, classify the following:\n",
        "        Text: {input_text}\n",
        "        Sentiment:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chain = few_shot_prompt | llm\n",
        "    result = chain.invoke(input_text).content\n",
        "\n",
        "    # Clean up the result\n",
        "    result = result.strip()\n",
        "    # Extract only the sentiment label\n",
        "    if ':' in result:\n",
        "        result = result.split(':')[1].strip()\n",
        "\n",
        "    return result  # This will now return just \"Positive\", \"Negative\", or \"Neutral\"\n",
        "\n",
        "test_text = \"I can't believe how great this new restaurant is!\"\n",
        "result = few_shot_sentiment_classification(test_text)\n",
        "print(f\"Input: {test_text}\")\n",
        "print(f\"Predicted Sentiment: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuXdAjoX2Ivs"
      },
      "source": [
        "## Advanced Few-Shot Techniques\n",
        "\n",
        "Multi-task learning for sentiment analysis and language detection.\n",
        "\n",
        "Multi-task Learning:\n",
        "- Definition: Training a model to perform multiple related tasks simultaneously.\n",
        "- Benefits: Improved efficiency, better generalization, reduced overfitting.\n",
        "\n",
        "Implementation:\n",
        "1. Design a prompt template that includes examples for multiple tasks.\n",
        "2. Use task-specific instructions to guide the model's behavior.\n",
        "3. Demonstrate how the same model can switch between tasks based on input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_vrJGid2Ivs",
        "outputId": "8c7b8623-f0eb-4d26-eabf-5003ac8adbd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive\n",
            "Result: German\n"
          ]
        }
      ],
      "source": [
        "def multi_task_few_shot(input_text, task):\n",
        "    few_shot_prompt = PromptTemplate(\n",
        "        input_variables=[\"input_text\", \"task\"],\n",
        "        template=\"\"\"\n",
        "        Perform the specified task on the given text.\n",
        "\n",
        "        Examples:\n",
        "        Text: I love this product! It's amazing.\n",
        "        Task: sentiment\n",
        "        Result: Positive\n",
        "\n",
        "        Text: Bonjour, comment allez-vous?\n",
        "        Task: language\n",
        "        Result: French\n",
        "\n",
        "        Now, perform the following task:\n",
        "        Text: {input_text}\n",
        "        Task: {task}\n",
        "        Result:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chain = few_shot_prompt | llm\n",
        "    return chain.invoke({\"input_text\": input_text, \"task\": task}).content\n",
        "\n",
        "print(multi_task_few_shot(\"I can't believe how great this is!\", \"sentiment\"))\n",
        "print(multi_task_few_shot(\"Guten Tag, wie geht es Ihnen?\", \"language\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq2S_9VW2Ivs"
      },
      "source": [
        "## In-Context Learning\n",
        "\n",
        "In-Context Learning allows models to adapt to new tasks based on examples provided in the prompt.\n",
        "\n",
        "Key Aspects:\n",
        "1. No fine-tuning required: The model learns from examples in the prompt.\n",
        "2. Flexibility: Can be applied to a wide range of tasks.\n",
        "3. Prompt engineering: Careful design of prompts is crucial for performance.\n",
        "\n",
        "Example Implementation:\n",
        "We'll demonstrate in-context learning for a custom task (converting text to pig latin)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXxVsnSE2Ivs",
        "outputId": "0588fe17-224a-4fd1-dc6d-2437ef7ceb73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: python\n",
            "Output: Output: ythonpay\n"
          ]
        }
      ],
      "source": [
        "def in_context_learning(task_description, examples, input_text):\n",
        "    example_text = \"\".join([f\"Input: {e['input']}\\nOutput: {e['output']}\\n\\n\" for e in examples])\n",
        "\n",
        "    in_context_prompt = PromptTemplate(\n",
        "        input_variables=[\"task_description\", \"examples\", \"input_text\"],\n",
        "        template=\"\"\"\n",
        "        Task: {task_description}\n",
        "\n",
        "        Examples:\n",
        "        {examples}\n",
        "\n",
        "        Now, perform the task on the following input:\n",
        "        Input: {input_text}\n",
        "        Output:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chain = in_context_prompt | llm\n",
        "    return chain.invoke({\"task_description\": task_description, \"examples\": example_text, \"input_text\": input_text}).content\n",
        "\n",
        "task_desc = \"Convert the given text to pig latin.\"\n",
        "examples = [\n",
        "    {\"input\": \"hello\", \"output\": \"ellohay\"},\n",
        "    {\"input\": \"apple\", \"output\": \"appleay\"}\n",
        "]\n",
        "test_input = \"python\"\n",
        "\n",
        "result = in_context_learning(task_desc, examples, test_input)\n",
        "print(f\"Input: {test_input}\")\n",
        "print(f\"Output: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCxnf-6X2Ivs"
      },
      "source": [
        "## Best Practices and Evaluation\n",
        "\n",
        "To maximize the effectiveness of few-shot and in-context learning:\n",
        "\n",
        "1. Example Selection:\n",
        "   - Diversity: Cover different aspects of the task.\n",
        "   - Clarity: Use unambiguous examples.\n",
        "   - Relevance: Choose examples similar to expected inputs.\n",
        "   - Balance: Ensure equal representation of classes/categories.\n",
        "   - Edge cases: Include examples of unusual or difficult cases.\n",
        "\n",
        "2. Prompt Engineering:\n",
        "   - Clear instructions: Specify the task explicitly.\n",
        "   - Consistent format: Maintain a uniform structure for examples and inputs.\n",
        "   - Conciseness: Avoid unnecessary information that may confuse the model.\n",
        "\n",
        "3. Evaluation:\n",
        "   - Create a diverse test set.\n",
        "   - Compare model predictions to true labels.\n",
        "   - Use appropriate metrics (e.g., accuracy, F1 score) based on the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDv10txc2Ivt",
        "outputId": "3f66a58a-8043-445d-90b5-a6c905ffb90a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: This product exceeded my expectations!\n",
            "Predicted: Positive\n",
            "Actual: Positive\n",
            "Correct: True\n",
            "\n",
            "Input: I'm utterly disappointed with the service.\n",
            "Predicted: Negative\n",
            "Actual: Negative\n",
            "Correct: True\n",
            "\n",
            "Input: The temperature today is 72 degrees.\n",
            "Predicted: Neutral\n",
            "Actual: Neutral\n",
            "Correct: True\n",
            "\n",
            "Model Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model_func, test_cases):\n",
        "    '''\n",
        "    Evaluate the model on a set of test cases.\n",
        "\n",
        "    Args:\n",
        "    model_func: The function that makes predictions.\n",
        "    test_cases: A list of dictionaries, where each dictionary contains an \"input\" text and a \"label\" for the input.\n",
        "\n",
        "    Returns:\n",
        "    The accuracy of the model on the test cases.\n",
        "    '''\n",
        "    correct = 0\n",
        "    total = len(test_cases)\n",
        "\n",
        "    for case in test_cases:\n",
        "        input_text = case['input']\n",
        "        true_label = case['label']\n",
        "        prediction = model_func(input_text).strip()\n",
        "\n",
        "        is_correct = prediction.lower() == true_label.lower()\n",
        "        correct += int(is_correct)\n",
        "\n",
        "        print(f\"Input: {input_text}\")\n",
        "        print(f\"Predicted: {prediction}\")\n",
        "        print(f\"Actual: {true_label}\")\n",
        "        print(f\"Correct: {is_correct}\\n\")\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "test_cases = [\n",
        "    {\"input\": \"This product exceeded my expectations!\", \"label\": \"Positive\"},\n",
        "    {\"input\": \"I'm utterly disappointed with the service.\", \"label\": \"Negative\"},\n",
        "    {\"input\": \"The temperature today is 72 degrees.\", \"label\": \"Neutral\"}\n",
        "]\n",
        "\n",
        "accuracy = evaluate_model(few_shot_sentiment_classification, test_cases)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}